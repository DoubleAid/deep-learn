# 优化器

## SGD (Stochastic Gradient Descent)

随机梯度下降是最基本的优化算法，它使用随机的小批量数据来计算提督并更新参数

```python
optimizer = torch.optim.SGD(model.parameters(), lr-0.01, momentum=0.9)
```

特点：可选的加入动量来加速SGD在相关方向上的收敛，并抑制震荡

## Adam (Adaptive Moment Estimation)

Adam 是一种自适应学习率的优化算法，结合了AdaGrad和RMSProp的优点

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

自动调整学习率，并保持过去梯度的指数衰减平均，通常在许多情况下都表现良好

## AdamW

AdamW 是 Adam 的一个变种，其中权重衰减与优化器逻辑解耦，以解决 L2 正则化在Adam算法中的不一致问题

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
```

通常在深度学习模型中使用，特别是在使用正则化时。

## RMSprop

RMSprop 是由 Geoff Hinton 提出的一种自适应学习率方法，旨在解决 AdaGrad 学习率急剧下降的问题。

```python
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)
```

适用于处理非平稳目标的优化问题，对 RNN 效果良好。

